{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from lib import datasets_path\n",
    "#from .pd_dataset import PandasDataset\n",
    "from ..utils.utils import disjoint_months, infer_mask, compute_mean, geographical_distance, thresholded_gaussian_kernel\n",
    "\n",
    "\n",
    "class AirQuality(PandasDataset):\n",
    "    SEED = 3210\n",
    "\n",
    "    def __init__(self, impute_nans=False, small=False, freq='60T', masked_sensors=None):\n",
    "        self.random = np.random.default_rng(self.SEED)\n",
    "        self.test_months = [3, 6, 9, 12]\n",
    "        self.infer_eval_from = 'next'\n",
    "        self.eval_mask = None\n",
    "        df, dist, mask = self.load(impute_nans=impute_nans, small=small, masked_sensors=masked_sensors)\n",
    "        self.dist = dist\n",
    "        if masked_sensors is None:\n",
    "            self.masked_sensors = list()\n",
    "        else:\n",
    "            self.masked_sensors = list(masked_sensors)\n",
    "        super().__init__(dataframe=df, u=None, mask=mask, name='air', freq=freq, aggr='nearest')\n",
    "\n",
    "    def load_raw(self, small=False):\n",
    "        if small:\n",
    "            path = os.path.join(datasets_path['air'], 'small36.h5')\n",
    "            eval_mask = pd.DataFrame(pd.read_hdf(path, 'eval_mask'))\n",
    "        else:\n",
    "            path = os.path.join(datasets_path['air'], 'full437.h5')\n",
    "            eval_mask = None\n",
    "        df = pd.DataFrame(pd.read_hdf(path, 'pm25'))\n",
    "        stations = pd.DataFrame(pd.read_hdf(path, 'stations'))\n",
    "        return df, stations, eval_mask\n",
    "\n",
    "    def load(self, impute_nans=True, small=False, masked_sensors=None):\n",
    "        # load readings and stations metadata\n",
    "        df, stations, eval_mask = self.load_raw(small)\n",
    "        # compute the masks\n",
    "        mask = (~np.isnan(df.values)).astype('uint8')  # 1 if value is not nan else 0\n",
    "        if eval_mask is None:\n",
    "            eval_mask = infer_mask(df, infer_from=self.infer_eval_from)\n",
    "\n",
    "        eval_mask = eval_mask.values.astype('uint8')\n",
    "        if masked_sensors is not None:\n",
    "            eval_mask[:, masked_sensors] = np.where(mask[:, masked_sensors], 1, 0)\n",
    "        self.eval_mask = eval_mask  # 1 if value is ground-truth for imputation else 0\n",
    "        # eventually replace nans with weekly mean by hour\n",
    "        if impute_nans:\n",
    "            df = df.fillna(compute_mean(df))\n",
    "        # compute distances from latitude and longitude degrees\n",
    "        st_coord = stations.loc[:, ['latitude', 'longitude']]\n",
    "        dist = geographical_distance(st_coord, to_rad=True).values\n",
    "        return df, dist, mask\n",
    "\n",
    "    def splitter(self, dataset, val_len=1., in_sample=False, window=0):\n",
    "        nontest_idxs, test_idxs = disjoint_months(dataset, months=self.test_months, synch_mode='horizon')\n",
    "        if in_sample:\n",
    "            train_idxs = np.arange(len(dataset))\n",
    "            val_months = [(m - 1) % 12 for m in self.test_months]\n",
    "            _, val_idxs = disjoint_months(dataset, months=val_months, synch_mode='horizon')\n",
    "        else:\n",
    "            # take equal number of samples before each month of testing\n",
    "            val_len = (int(val_len * len(nontest_idxs)) if val_len < 1 else val_len) // len(self.test_months)\n",
    "            # get indices of first day of each testing month\n",
    "            delta_idxs = np.diff(test_idxs)\n",
    "            end_month_idxs = test_idxs[1:][np.flatnonzero(delta_idxs > delta_idxs.min())]\n",
    "            if len(end_month_idxs) < len(self.test_months):\n",
    "                end_month_idxs = np.insert(end_month_idxs, 0, test_idxs[0])\n",
    "            # expand month indices\n",
    "            month_val_idxs = [np.arange(v_idx - val_len, v_idx) - window for v_idx in end_month_idxs]\n",
    "            val_idxs = np.concatenate(month_val_idxs) % len(dataset)\n",
    "            # remove overlapping indices from training set\n",
    "            ovl_idxs, _ = dataset.overlapping_indices(nontest_idxs, val_idxs, synch_mode='horizon', as_mask=True)\n",
    "            train_idxs = nontest_idxs[~ovl_idxs]\n",
    "        return [train_idxs, val_idxs, test_idxs]\n",
    "\n",
    "    def get_similarity(self, thr=0.1, include_self=False, force_symmetric=False, sparse=False, **kwargs):\n",
    "        theta = np.std(self.dist[:36, :36])  # use same theta for both air and air36\n",
    "        adj = thresholded_gaussian_kernel(self.dist, theta=theta, threshold=thr)\n",
    "        if not include_self:\n",
    "            adj[np.diag_indices_from(adj)] = 0.\n",
    "        if force_symmetric:\n",
    "            adj = np.maximum.reduce([adj, adj.T])\n",
    "        if sparse:\n",
    "            import scipy.sparse as sps\n",
    "            adj = sps.coo_matrix(adj)\n",
    "        return adj\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "    @property\n",
    "    def training_mask(self):\n",
    "        return self._mask if self.eval_mask is None else (self._mask & (1 - self.eval_mask))\n",
    "\n",
    "    def test_interval_mask(self, dtype=bool, squeeze=True):\n",
    "        m = np.in1d(self.df.index.month, self.test_months).astype(dtype)\n",
    "        if squeeze:\n",
    "            return m\n",
    "        return m[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20119f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e982825",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = {\n",
    "    'air': 'datasets/air_quality',\n",
    "    'la': 'datasets/metr_la',\n",
    "    'bay': 'datasets/pems_bay',\n",
    "    'synthetic': 'datasets/synthetic',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c58cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset:\n",
    "    def __init__(self, dataframe: pd.DataFrame, u: pd.DataFrame = None, name='pd-dataset', mask=None, freq=None,\n",
    "                 aggr='sum', **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a tsl dataset from a pandas dataframe.\n",
    "\n",
    "\n",
    "        :param dataframe: dataframe containing the data, shape: n_steps, n_nodes\n",
    "        :param u: dataframe with exog variables\n",
    "        :param name: optional name of the dataset\n",
    "        :param mask: mask for valid data (1:valid, 0:not valid)\n",
    "        :param freq: force a frequency (possibly by resampling)\n",
    "        :param aggr: aggregation method after resampling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "        # set dataset dataframe\n",
    "        self.df = dataframe\n",
    "\n",
    "        # set optional exog_variable dataframe\n",
    "        # make sure to consider only the overlapping part of the two dataframes\n",
    "        # assumption u.index \\in df.index\n",
    "        idx = sorted(self.df.index)\n",
    "        self.start = idx[0]\n",
    "        self.end = idx[-1]\n",
    "\n",
    "        if u is not None:\n",
    "            self.u = u[self.start:self.end]\n",
    "        else:\n",
    "            self.u = None\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = np.asarray(mask).astype('uint8')\n",
    "        self._mask = mask\n",
    "\n",
    "        if freq is not None:\n",
    "            self.resample_(freq=freq, aggr=aggr)\n",
    "        else:\n",
    "            self.freq = self.df.index.inferred_freq\n",
    "            # make sure that all the dataframes are aligned\n",
    "            self.resample_(self.freq, aggr=aggr)\n",
    "\n",
    "        assert 'T' in self.freq\n",
    "        self.samples_per_day = int(60 / int(self.freq[:-1]) * 24)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(nodes={}, length={})\".format(self.__class__.__name__, self.n_nodes, self.length)\n",
    "\n",
    "    @property\n",
    "    def has_mask(self):\n",
    "        return self._mask is not None\n",
    "\n",
    "    @property\n",
    "    def has_u(self):\n",
    "        return self.u is not None\n",
    "\n",
    "    def resample_(self, freq, aggr):\n",
    "        resampler = self.df.resample(freq)\n",
    "        idx = self.df.index\n",
    "        if aggr == 'sum':\n",
    "            self.df = resampler.sum()\n",
    "        elif aggr == 'mean':\n",
    "            self.df = resampler.mean()\n",
    "        elif aggr == 'nearest':\n",
    "            self.df = resampler.nearest()\n",
    "        else:\n",
    "            raise ValueError(f'{aggr} if not a valid aggregation method.')\n",
    "\n",
    "        if self.has_mask:\n",
    "            resampler = pd.DataFrame(self._mask, index=idx).resample(freq)\n",
    "            self._mask = resampler.min().to_numpy()\n",
    "\n",
    "        if self.has_u:\n",
    "            resampler = self.u.resample(freq)\n",
    "            self.u = resampler.nearest()\n",
    "        self.freq = freq\n",
    "\n",
    "    def dataframe(self) -> pd.DataFrame:\n",
    "        return self.df.copy()\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.df.values.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_nodes(self):\n",
    "        return self.df.values.shape[1]\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        if self._mask is None:\n",
    "            return np.ones_like(self.df.values).astype('uint8')\n",
    "        return self._mask\n",
    "\n",
    "    def numpy(self, return_idx=False):\n",
    "        if return_idx:\n",
    "            return self.numpy(), self.df.index\n",
    "        return self.df.values\n",
    "\n",
    "    def pytorch(self):\n",
    "        data = self.numpy()\n",
    "        return torch.FloatTensor(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_raw(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geographical_distance(x=None, to_rad=True):\n",
    "    \"\"\"\n",
    "    Compute the as-the-crow-flies distance between every pair of samples in `x`. The first dimension of each point is\n",
    "    assumed to be the latitude, the second is the longitude. The inputs is assumed to be in degrees. If it is not the\n",
    "    case, `to_rad` must be set to False. The dimension of the data must be 2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.DataFrame or np.ndarray\n",
    "        array_like structure of shape (n_samples_2, 2).\n",
    "    to_rad : bool\n",
    "        whether to convert inputs to radians (provided that they are in degrees).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distances :\n",
    "        The distance between the points in kilometers.\n",
    "    \"\"\"\n",
    "    _AVG_EARTH_RADIUS_KM = 6371.0088\n",
    "\n",
    "    # Extract values of X if it is a DataFrame, else assume it is 2-dim array of lat-lon pairs\n",
    "    latlon_pairs = x.values if isinstance(x, pd.DataFrame) else x\n",
    "\n",
    "    # If the input values are in degrees, convert them in radians\n",
    "    if to_rad:\n",
    "        latlon_pairs = np.vectorize(np.radians)(latlon_pairs)\n",
    "\n",
    "    distances = haversine_distances(latlon_pairs) * _AVG_EARTH_RADIUS_KM\n",
    "\n",
    "    # Cast response\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        res = pd.DataFrame(distances, x.index, x.index)\n",
    "    else:\n",
    "        res = distances\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(x, index=None):\n",
    "    \"\"\"Compute the mean values for each datetime. The mean is first computed hourly over the week of the year.\n",
    "    Further NaN values are computed using hourly mean over the same month through the years. If other NaN are present,\n",
    "    they are removed using the mean of the sole hours. Hoping reasonably that there is at least a non-NaN entry of the\n",
    "    same hour of the NaN datetime in all the dataset.\"\"\"\n",
    "    if isinstance(x, np.ndarray) and index is not None:\n",
    "        shape = x.shape\n",
    "        x = x.reshape((shape[0], -1))\n",
    "        df_mean = pd.DataFrame(x, index=index)\n",
    "    else:\n",
    "        df_mean = x.copy()\n",
    "    cond0 = [df_mean.index.year, df_mean.index.isocalendar().week, df_mean.index.hour]\n",
    "    cond1 = [df_mean.index.year, df_mean.index.month, df_mean.index.hour]\n",
    "    conditions = [cond0, cond1, cond1[1:], cond1[2:]]\n",
    "    while df_mean.isna().values.sum() and len(conditions):\n",
    "        nan_mean = df_mean.groupby(conditions[0]).transform(np.nanmean)\n",
    "        df_mean = df_mean.fillna(nan_mean)\n",
    "        conditions = conditions[1:]\n",
    "    if df_mean.isna().values.sum():\n",
    "        df_mean = df_mean.fillna(method='ffill')\n",
    "        df_mean = df_mean.fillna(method='bfill')\n",
    "    if isinstance(x, np.ndarray):\n",
    "        df_mean = df_mean.values.reshape(shape)\n",
    "    return df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(small=False):\n",
    "    if small:\n",
    "        path = os.path.join(datasets_path['air'], 'small36.h5')\n",
    "        eval_mask = pd.DataFrame(pd.read_hdf(path, 'eval_mask'))\n",
    "    else:\n",
    "        path = os.path.join(datasets_path['air'], 'full437.h5')\n",
    "        eval_mask = None\n",
    "    df = pd.DataFrame(pd.read_hdf(path, 'pm25'))\n",
    "    stations = pd.DataFrame(pd.read_hdf(path, 'stations'))\n",
    "    return df, stations, eval_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be94cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(impute_nans=True, small=False, masked_sensors=None):\n",
    "    # load readings and stations metadata\n",
    "    df, stations, eval_mask = load_raw(small)\n",
    "    # compute the masks\n",
    "    mask = (~np.isnan(df.values)).astype('uint8')  # 1 if value is not nan else 0\n",
    "    if eval_mask is None:\n",
    "        eval_mask = infer_mask(df, infer_from='next')\n",
    "    eval_mask = eval_mask.values.astype('uint8')\n",
    "    if masked_sensors is not None:\n",
    "        eval_mask[:, masked_sensors] = np.where(mask[:, masked_sensors], 1, 0)\n",
    "    #self.eval_mask = eval_mask  # 1 if value is ground-truth for imputation else 0\n",
    "        # eventually replace nans with weekly mean by hour\n",
    "    if impute_nans:\n",
    "        df = df.fillna(compute_mean(df))\n",
    "        # compute distances from latitude and longitude degrees\n",
    "    st_coord = stations.loc[:, ['latitude', 'longitude']]\n",
    "    dist = geographical_distance(st_coord, to_rad=True).values\n",
    "    return df, dist, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f001a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, stations, eval_mask = load_raw(small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d238107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a530fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcec378",
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_mask == 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dist, mask = load(impute_nans=False, small=True, masked_sensors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad44231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholded_gaussian_kernel(x, theta=None, threshold=None, threshold_on_input=False):\n",
    "    if theta is None:\n",
    "        theta = np.std(x)\n",
    "    weights = np.exp(-np.square(x / theta))\n",
    "    if threshold is not None:\n",
    "        mask = x > threshold if threshold_on_input else weights < threshold\n",
    "        weights[mask] = 0.\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ee236",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_similarity(dist, thr=0.1, include_self=False, force_symmetric=False, sparse=False, **kwargs):\n",
    "        theta = np.std(dist[:36, :36])  # use same theta for both air and air36\n",
    "        adj = thresholded_gaussian_kernel(dist, theta=theta, threshold=thr)\n",
    "        if not include_self:\n",
    "            adj[np.diag_indices_from(adj)] = 0.\n",
    "        if force_symmetric:\n",
    "            adj = np.maximum.reduce([adj, adj.T])\n",
    "        if sparse:\n",
    "            import scipy.sparse as sps\n",
    "            adj = sps.coo_matrix(adj)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9385f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj=get_similarity(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3384bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirQuality(PandasDataset):\n",
    "    SEED = 3210\n",
    "\n",
    "    def __init__(self, impute_nans=False, small=False, freq='60T', masked_sensors=None):\n",
    "        self.random = np.random.default_rng(self.SEED)\n",
    "        self.test_months = [3, 6, 9, 12]\n",
    "        self.infer_eval_from = 'next'\n",
    "        self.eval_mask = None\n",
    "        df, dist, mask = self.load(impute_nans=impute_nans, small=small, masked_sensors=masked_sensors)\n",
    "        self.dist = dist\n",
    "        if masked_sensors is None:\n",
    "            self.masked_sensors = list()\n",
    "        else:\n",
    "            self.masked_sensors = list(masked_sensors)\n",
    "        super().__init__(dataframe=df, u=None, mask=mask, name='air', freq=freq, aggr='nearest')\n",
    "\n",
    "    def load_raw(self, small=False):\n",
    "        if small:\n",
    "            path = os.path.join(datasets_path['air'], 'small36.h5')\n",
    "            eval_mask = pd.DataFrame(pd.read_hdf(path, 'eval_mask'))\n",
    "        else:\n",
    "            path = os.path.join(datasets_path['air'], 'full437.h5')\n",
    "            eval_mask = None\n",
    "        df = pd.DataFrame(pd.read_hdf(path, 'pm25'))\n",
    "        stations = pd.DataFrame(pd.read_hdf(path, 'stations'))\n",
    "        return df, stations, eval_mask\n",
    "\n",
    "    def load(self, impute_nans=True, small=False, masked_sensors=None):\n",
    "        # load readings and stations metadata\n",
    "        df, stations, eval_mask = self.load_raw(small)\n",
    "        # compute the masks\n",
    "        mask = (~np.isnan(df.values)).astype('uint8')  # 1 if value is not nan else 0\n",
    "        if eval_mask is None:\n",
    "            eval_mask = infer_mask(df, infer_from=self.infer_eval_from)\n",
    "\n",
    "        eval_mask = eval_mask.values.astype('uint8')\n",
    "        if masked_sensors is not None:\n",
    "            eval_mask[:, masked_sensors] = np.where(mask[:, masked_sensors], 1, 0)\n",
    "        self.eval_mask = eval_mask  # 1 if value is ground-truth for imputation else 0\n",
    "        # eventually replace nans with weekly mean by hour\n",
    "        if impute_nans:\n",
    "            df = df.fillna(compute_mean(df))\n",
    "        # compute distances from latitude and longitude degrees\n",
    "        st_coord = stations.loc[:, ['latitude', 'longitude']]\n",
    "        dist = geographical_distance(st_coord, to_rad=True).values\n",
    "        return df, dist, mask\n",
    "\n",
    "    def splitter(self, dataset, val_len=1., in_sample=False, window=0):\n",
    "        nontest_idxs, test_idxs = disjoint_months(dataset, months=self.test_months, synch_mode='horizon')\n",
    "        if in_sample:\n",
    "            train_idxs = np.arange(len(dataset))\n",
    "            val_months = [(m - 1) % 12 for m in self.test_months]\n",
    "            _, val_idxs = disjoint_months(dataset, months=val_months, synch_mode='horizon')\n",
    "        else:\n",
    "            # take equal number of samples before each month of testing\n",
    "            val_len = (int(val_len * len(nontest_idxs)) if val_len < 1 else val_len) // len(self.test_months)\n",
    "            # get indices of first day of each testing month\n",
    "            delta_idxs = np.diff(test_idxs)\n",
    "            end_month_idxs = test_idxs[1:][np.flatnonzero(delta_idxs > delta_idxs.min())]\n",
    "            if len(end_month_idxs) < len(self.test_months):\n",
    "                end_month_idxs = np.insert(end_month_idxs, 0, test_idxs[0])\n",
    "            # expand month indices\n",
    "            month_val_idxs = [np.arange(v_idx - val_len, v_idx) - window for v_idx in end_month_idxs]\n",
    "            val_idxs = np.concatenate(month_val_idxs) % len(dataset)\n",
    "            # remove overlapping indices from training set\n",
    "            ovl_idxs, _ = dataset.overlapping_indices(nontest_idxs, val_idxs, synch_mode='horizon', as_mask=True)\n",
    "            train_idxs = nontest_idxs[~ovl_idxs]\n",
    "        return [train_idxs, val_idxs, test_idxs]\n",
    "\n",
    "    def get_similarity(self, thr=0.1, include_self=False, force_symmetric=False, sparse=False, **kwargs):\n",
    "        theta = np.std(self.dist[:36, :36])  # use same theta for both air and air36\n",
    "        adj = thresholded_gaussian_kernel(self.dist, theta=theta, threshold=thr)\n",
    "        if not include_self:\n",
    "            adj[np.diag_indices_from(adj)] = 0.\n",
    "        if force_symmetric:\n",
    "            adj = np.maximum.reduce([adj, adj.T])\n",
    "        if sparse:\n",
    "            import scipy.sparse as sps\n",
    "            adj = sps.coo_matrix(adj)\n",
    "        return adj\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "    @property\n",
    "    def training_mask(self):\n",
    "        return self._mask if self.eval_mask is None else (self._mask & (1 - self.eval_mask))\n",
    "\n",
    "    def test_interval_mask(self, dtype=bool, squeeze=True):\n",
    "        m = np.in1d(self.df.index.month, self.test_months).astype(dtype)\n",
    "        if squeeze:\n",
    "            return m\n",
    "        return m[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b22d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.AirQuality(impute_nans=True, small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_mask=eval_mask.iloc[2300:4100,1:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_mask.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913067c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cb768",
   "metadata": {},
   "source": [
    "discharge data read in and apply mask, then export h5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship = pd.read_csv('discharge/relationshipset2.csv',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34764be",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge = pd.read_csv('discharge/set2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge=discharge.set_index('datetime')#将date作为标签\n",
    "discharge.index=pd.DatetimeIndex(discharge.index)#将标签转为时间索引\n",
    "discharge.axes#查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge1800=discharge[-1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe71218",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=discharge1800.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_eval_mask=pd.DataFrame(new_eval_mask).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mask.shape[1]):\n",
    "    mask.iloc[:,i]=new_eval_mask[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32625908",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mask == 0).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6755eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=pd.DataFrame(np.zeros([mask.shape[1],mask.shape[1]]),columns=mask.columns,index=mask.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aeef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(relationship.shape[0]):\n",
    "    dist.loc[relationship.iloc[i,1],relationship.iloc[i,0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40abbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dist == 1).astype(int).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78394aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('water.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put(key='eval_mask',value=mask);\n",
    "store.put(key='discharge',value=discharge1800);\n",
    "store.put(key='dist',value=dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_water():\n",
    "    path='water.h5'\n",
    "    eval_mask = pd.DataFrame(pd.read_hdf(path, 'eval_mask'))\n",
    "    df = pd.DataFrame(pd.read_hdf(path, 'discharge'))\n",
    "    dist = pd.DataFrame(pd.read_hdf(path, 'dist'))\n",
    "    return df, dist, eval_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a167159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dist, eval_mask = load_raw_water()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a997e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_array=dist.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd19aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73636ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waterpy39",
   "language": "python",
   "name": "waterpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
